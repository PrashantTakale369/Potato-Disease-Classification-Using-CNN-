# -*- coding: utf-8 -*-
"""Potato_Disease_Model_Building.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s0MSzkZxD8uNTHU6OUcUcuGMak-KK22X
"""

import tensorflow as tf
import tensorflow as tf
from tensorflow.keras import models,layers
import matplotlib.pyplot as plt

"""Set all the Constants"""

BATCH_SIZE = 32
IMAGE_SIZE = 256
CHANNELS=3
EPOCHS=50

dataset = tf.keras.preprocessing.image_dataset_from_directory(
    "/content/drive/MyDrive/Training/Potato",
    seed=123,
    shuffle=True,
    image_size=(IMAGE_SIZE,IMAGE_SIZE),  # Width or Height  256 * 256
    batch_size=BATCH_SIZE
)

class_names = dataset.class_names
class_names

for image_batch, labels_batch in dataset.take(1):
    print(image_batch.shape)
    print(labels_batch.numpy())

plt.figure(figsize=(15, 15))                                   # ploting the image here
for image_batch, labels_batch in dataset.take(1):
    for i in range(12):
        ax = plt.subplot(4, 4, i + 1)
        plt.imshow(image_batch[i].numpy().astype("uint8"))
        plt.title(class_names[labels_batch[i]])
        plt.axis("off")

type(image_batch[i])

type(image_batch[i].numpy())

# * Function to Split Dataset
#   Dataset should be bifurcated into 3 subsets, namely:

# 1. Training: Dataset to be used  while training
# 2. Validation: Dataset to be tested against while training
# 3. Test: Dataset to be tested against after we trained a model

len(dataset)

train_size = 0.8
len(dataset)*train_size

# Total Data Will be The 80 % And then we do the further process which is like :

# Traning Data = 80%
# Testing Data = 20%
#                ==> test data       = 10%
#                ==> valdation data  = 10%

"""this is the One By One trian and Test Split"""

train_ds = dataset.take(54)            #  Traning Data
len(train_ds)

test_ds = dataset.skip(54)             # Testing Data

len(test_ds)

val_ds = test_ds.take(6)                # Validation Data
len(val_ds)

test_ds = test_ds.skip(6)
len(test_ds)

"""Creating the Fun For Traning and Teting Data"""

def get_data_partitions_tf(ds,train_split=0.8,val_split =0.1,test_split =0.1,shuffle=True,shuffle_size=10000):

    assert (train_split + test_split + val_split) == 1

    ds_size = len(ds)

    if shuffle:
      ds = ds.shuffle(shuffle_size, seed=12)

    train_size = int(train_split * ds_size)                    # this is the Fun creating for the split data into train and test
    val_size = int(val_split * ds_size)

    train_ds = ds.take(train_size)
    val_ds = ds.skip(train_size).take(val_size)
    test_ds = ds.skip(train_size).skip(val_size)



    return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_data_partitions_tf(dataset)

len(train_ds)

len(val_ds)

len(test_ds)

"""Cache, Shuffle, and Prefetch the Dataset"""

train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)    # we are doing here cache , prefetch and shuffel on data
val_ds = val_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)      # in the Aboev redme file i will give the full info
test_ds = test_ds.cache().shuffle(1000).prefetch(buffer_size=tf.data.AUTOTUNE)

"""Normalizing the Image bcz the rang is 0-255"""

for image_batch, labels_batch in dataset.take(1):   # calculating the image shape
    print(image_batch[0].numpy().shape)

for image_batch, labels_batch in dataset.take(1):     # 1 image in numpay how look like this is demonstration
    print(image_batch[0].numpy())

"""# Resizing and Normalization

```

Creating a Layer for Resizing and Normalization
Before we feed our images to network, we should be resizing it
to the desired size. Moreover, to improve model performance, we
should normalize the image pixel value (keeping them in range 0
and 1 by dividing by 256). This should happen while training as
well as inference. Hence we can add that as a layer in our
Sequential Model.

You might be thinking why do we need to resize (256,256) image
to again (256,256). You are right we don't need to but this will
be useful when we are done with the training and start using the
model for predictions. At that time somone can supply an image
that is not (256,256) and this layer will resize it
```
"""

from tensorflow.keras import layers
from tensorflow.keras.layers import Normalization
from tensorflow.keras.layers import Rescaling, Normalization

resize_and_rescale = tf.keras.Sequential([
  layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),
  layers.Rescaling(1.0/255),
])

"""# Data Augmentation"""

data_augmentation = tf.keras.Sequential([
  layers.RandomFlip("horizontal_and_vertical"),
  layers.RandomRotation(0.2),
  layers.RandomZoom(0.2),
  layers.RandomContrast(0.2)
])

"""# Model HP Tunning"""

pip install keras_tuner

# from keras_tuner import RandomSearch
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# def build_model(hp):

#     model = Sequential()

#     kernel_size = hp.Int('kernel_size', 3, 5, step=2)

#     kernel_choice = hp.Choice('kernel_size', ['3x3', '5x5'])
#     if kernel_choice == '3x3':
#       kernel_size = (3, 3)
#     else:
#       kernel_size = (5, 5)

#     # Conv Layer

#     model.add(Conv2D(
#         filters=hp.Choice('conv_filters', [32, 64, 128]),
#         kernel_size=hp.Choice('kernel_size', [(3,3), (5,5)]),
#         activation='relu',
#         input_shape=(128, 128, 3)
#     ))

#     model.add(MaxPooling2D(pool_size=(2, 2)))

#     # Dropout Layer
#     model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.5, step=0.1)))

#     model.add(Flatten())

#     # Dense Layer
#     model.add(Dense(hp.Int('dense_units', 64, 256, step=64), activation='relu'))

#     model.add(Dense(3, activation='softmax'))  # For 3 classes (Healthy, Early, Late)

#     model.compile(
#         optimizer='adam',
#         loss='sparse_categorical_crossentropy',
#         metrics=['accuracy']
#     )

#     return model

# tuner = RandomSearch(
#     build_model,
#     objective='val_accuracy',
#     max_trials=10,
#     directory='cnn_tuning',
#     project_name='potato_disease'
# )

# tuner.search(train_ds, validation_data=val_ds, epochs=10)

input_shape = (BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS)

n_classes = 3

model = models.Sequential([

    resize_and_rescale,
    layers.Conv2D(32, kernel_size = (3,3), activation='relu', input_shape=input_shape),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64,  kernel_size = (3,3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    layers.Flatten(),

    layers.Dense(64, activation='relu'),

    layers.Dense(n_classes, activation='softmax'),



])

model.build(input_shape=input_shape)

model.summary()

model.compile(
    optimizer='adam',
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics=['accuracy']
)

history = model.fit(
    train_ds,
    batch_size=BATCH_SIZE,
    validation_data=val_ds,
    verbose=1,
    epochs=35,
)



"""# Good Model (Well Generalized)"""

plt.plot(history.history['accuracy'], label='Training acc')
plt.plot(history.history['val_accuracy'], label='Validation acc')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title("Training vs Validation accuracy")
plt.show()

plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()

"""# how bad the model's predictions are"""

plt.plot(history.history['val_accuracy'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.title("Training vs Validation Loss")
plt.show()

len(test_ds)

acc = model.evaluate(test_ds)

acc

"""Testinf On data"""

import numpy as np

for image_batch,labels_batch in test_ds.take(1):

  first_image = image_batch[0].numpy().astype("uint8")

  print("Actual Label:",class_names[labels_batch[0].numpy()])

  plt.imshow(image_batch[0].numpy().astype("uint8"))

  print("first Image")
  batch_pred =  model.predict(image_batch)
  print("Predicted Label",class_names[np.argmax(batch_pred[0])])

"""# New Prediction Model Actual Wala"""

def predict(model, img):

    img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
    img_array = tf.expand_dims(img_array, 0)

    predictions = model.predict(img_array)

    predicted_class = class_names[np.argmax(predictions[0])]
    confidence = round(100 * (np.max(predictions[0])), 2)
    return predicted_class, confidence

plt.figure(figsize=(10, 13))
for images, labels in test_ds.take(1):
    for i in range(9):
        ax = plt.subplot(3, 3, i + 1)
        plt.imshow(images[i].numpy().astype("uint8"))

        predicted_class, confidence = predict(model, images[i].numpy())
        actual_class = class_names[labels[i]]

        plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class}.\n Confidence: {confidence}%")

        plt.axis("off")

"""# Saving the Model</p>
We append the model to the list of models as a new version
"""

import os

# Filter out non-integer directory names before converting to int
model_version=max([int(i) for i in os.listdir("/content/Potato_disease_Model") if i.isdigit()] + [0])+1

# Add the .keras extension to the filepath
model.save(f"/content/Potato_disease_Model/version_1{model_version}.keras")